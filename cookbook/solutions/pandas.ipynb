{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d265295f-455b-444b-a889-81d90dba6495",
   "metadata": {},
   "source": [
    "# Pandas: scaling to large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69bb0843-8a3e-4ad9-a086-1b8809199dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b601349-90df-491a-9d81-1efa8b522a94",
   "metadata": {},
   "source": [
    "Create a large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3913fa68-8958-4964-b167-c286d12f1397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 1.61 s, total: 1min 28s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def gen_random_string(length:int=32) -> str:\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))\n",
    "    \n",
    "def make_timeseries(start=\"2000-01-01\", end=\"2000-12-31\", freq=\"1D\", seed=None):\n",
    "\n",
    "    index = pd.date_range(start=start, end=end, freq=freq, name=\"timestamp\")\n",
    "    n = len(index)\n",
    "    np.random.seed = seed\n",
    "    columns = {\n",
    "        'cat': np.random.choice(['cat1','cat2','cat3','cat4','cat5'],n),\n",
    "        'str1':[gen_random_string() for _ in range(n)],\n",
    "        'str2':[gen_random_string() for _ in range(n)],\n",
    "        'a': np.random.rand(n),\n",
    "        'b': np.random.rand(n),\n",
    "        'c': np.random.randint(1,100,n),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(columns, index=index, columns=sorted(columns))\n",
    "    if df.index[-1] == end:\n",
    "        df = df.iloc[:-1]\n",
    "    return df\n",
    "\n",
    "timeseries = [\n",
    "    make_timeseries(start=datetime(2020,1,1), end=datetime(2023,12,31), freq='1min', seed=10).rename(columns=lambda x: f\"{x}_{i}\")\n",
    "    for i in range(5)\n",
    "]\n",
    "df = pd.concat(timeseries, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed3e1c-0dd5-4793-9f8f-78bd43591dec",
   "metadata": {},
   "source": [
    "Print the fisrt rows to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033a1a09-8469-4658-b37e-794bf63e6ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_0</th>\n",
       "      <th>b_0</th>\n",
       "      <th>c_0</th>\n",
       "      <th>cat_0</th>\n",
       "      <th>str1_0</th>\n",
       "      <th>str2_0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>c_1</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>...</th>\n",
       "      <th>c_3</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>str1_3</th>\n",
       "      <th>str2_3</th>\n",
       "      <th>a_4</th>\n",
       "      <th>b_4</th>\n",
       "      <th>c_4</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>str1_4</th>\n",
       "      <th>str2_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>0.439790</td>\n",
       "      <td>0.069992</td>\n",
       "      <td>85</td>\n",
       "      <td>cat1</td>\n",
       "      <td>25CH60DTU73CZ67GR54GS83RD6N3ARPP</td>\n",
       "      <td>8P63JISH1ZP47MIAZBI12V1JXGK58IX8</td>\n",
       "      <td>0.265880</td>\n",
       "      <td>0.903861</td>\n",
       "      <td>82</td>\n",
       "      <td>cat2</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>cat4</td>\n",
       "      <td>NBVW8J14DZTVEDHISETW5GAK7EF17GXG</td>\n",
       "      <td>OEZF17L5NT64ZJB0KDDPXLGZ65OZAHSO</td>\n",
       "      <td>0.687856</td>\n",
       "      <td>0.678377</td>\n",
       "      <td>34</td>\n",
       "      <td>cat4</td>\n",
       "      <td>TEJ5L414VFDEE80ZKN4MOTT55W84VIV1</td>\n",
       "      <td>04ZMT2BQ1A9LDVL5C8FAMYIH5UWBU0UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:01:00</th>\n",
       "      <td>0.307949</td>\n",
       "      <td>0.737754</td>\n",
       "      <td>28</td>\n",
       "      <td>cat4</td>\n",
       "      <td>WU6SF4XH0N297NGVQRVT4HJPRHFYY3K0</td>\n",
       "      <td>DHR0FWQD06X1373HHDCZIH7XVYN4ATNP</td>\n",
       "      <td>0.431852</td>\n",
       "      <td>0.119335</td>\n",
       "      <td>21</td>\n",
       "      <td>cat2</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>cat2</td>\n",
       "      <td>HZRH5W38UBQR9OV6BV3FY8EMTP33HSPB</td>\n",
       "      <td>HNUFOM1BLH8E30XVTFSFK13AZP23KX2Z</td>\n",
       "      <td>0.186750</td>\n",
       "      <td>0.882835</td>\n",
       "      <td>43</td>\n",
       "      <td>cat5</td>\n",
       "      <td>LAEIZ0WESDJ6JZ2E0M8LMQAH7ZBZEHY8</td>\n",
       "      <td>GQT42GP69K4YZ50H00I695KRSEBX87BB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:02:00</th>\n",
       "      <td>0.844226</td>\n",
       "      <td>0.251170</td>\n",
       "      <td>31</td>\n",
       "      <td>cat2</td>\n",
       "      <td>CXHNSBLSIIPQ9V5CBM1XA13APFKC4TX2</td>\n",
       "      <td>46YDT9RO6UUM9ZEVE537U2WH9M9YANX2</td>\n",
       "      <td>0.170822</td>\n",
       "      <td>0.954205</td>\n",
       "      <td>11</td>\n",
       "      <td>cat5</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>cat2</td>\n",
       "      <td>NI4WLNG33HG36JSWYSPJR1A411L86UYM</td>\n",
       "      <td>P6BULVBOV581SOARU9QTSQK8WW4HRKKS</td>\n",
       "      <td>0.864973</td>\n",
       "      <td>0.355677</td>\n",
       "      <td>71</td>\n",
       "      <td>cat5</td>\n",
       "      <td>3596BQIYQQ0FLWQDBRM9NFTRGJVOLPVH</td>\n",
       "      <td>AW7H0ELFQFU75NVEN5ZU5AK1FGV0LAG4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:03:00</th>\n",
       "      <td>0.509127</td>\n",
       "      <td>0.226031</td>\n",
       "      <td>22</td>\n",
       "      <td>cat2</td>\n",
       "      <td>24PBVSV29Z015YFR8XR0FM88YBXWGP52</td>\n",
       "      <td>X24BJX4X4H0SXXSTER0WY2OG8H51OPRP</td>\n",
       "      <td>0.437260</td>\n",
       "      <td>0.907608</td>\n",
       "      <td>42</td>\n",
       "      <td>cat4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>cat4</td>\n",
       "      <td>JN91A6OZKPZVOQ659KMWBCI0S314QDP3</td>\n",
       "      <td>JXZPV75E16IKN066HF1YEZN818R3DBA2</td>\n",
       "      <td>0.945028</td>\n",
       "      <td>0.491691</td>\n",
       "      <td>87</td>\n",
       "      <td>cat2</td>\n",
       "      <td>HF6EQ8TJ923PM3028ZER7812CV2M2HN4</td>\n",
       "      <td>AUGEW9I2R1L7EBJ1TJBK5V765S24SBKZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:04:00</th>\n",
       "      <td>0.845486</td>\n",
       "      <td>0.419131</td>\n",
       "      <td>78</td>\n",
       "      <td>cat5</td>\n",
       "      <td>PF8D3NWV0EP941UNREM9JO094PQNJDR9</td>\n",
       "      <td>HPO5JJA877ORFQXB6NWIG2MCRF2G38YP</td>\n",
       "      <td>0.385516</td>\n",
       "      <td>0.230385</td>\n",
       "      <td>12</td>\n",
       "      <td>cat4</td>\n",
       "      <td>...</td>\n",
       "      <td>72</td>\n",
       "      <td>cat4</td>\n",
       "      <td>ZSX13ZKIUQ1JB9V5QTM05D7O8SQJFOIN</td>\n",
       "      <td>9MVSK7KZ3FPJZYVKMOKQ9OF2UYZY2VJ6</td>\n",
       "      <td>0.328728</td>\n",
       "      <td>0.631368</td>\n",
       "      <td>3</td>\n",
       "      <td>cat2</td>\n",
       "      <td>2VMO6KZ0NR5YYBMUUJ4LZ2GUGFT1PQSX</td>\n",
       "      <td>GGOND089HZEJEXNECS4RMFP5PFA7JNT6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          a_0       b_0  c_0 cat_0  \\\n",
       "timestamp                                            \n",
       "2020-01-01 00:00:00  0.439790  0.069992   85  cat1   \n",
       "2020-01-01 00:01:00  0.307949  0.737754   28  cat4   \n",
       "2020-01-01 00:02:00  0.844226  0.251170   31  cat2   \n",
       "2020-01-01 00:03:00  0.509127  0.226031   22  cat2   \n",
       "2020-01-01 00:04:00  0.845486  0.419131   78  cat5   \n",
       "\n",
       "                                               str1_0  \\\n",
       "timestamp                                               \n",
       "2020-01-01 00:00:00  25CH60DTU73CZ67GR54GS83RD6N3ARPP   \n",
       "2020-01-01 00:01:00  WU6SF4XH0N297NGVQRVT4HJPRHFYY3K0   \n",
       "2020-01-01 00:02:00  CXHNSBLSIIPQ9V5CBM1XA13APFKC4TX2   \n",
       "2020-01-01 00:03:00  24PBVSV29Z015YFR8XR0FM88YBXWGP52   \n",
       "2020-01-01 00:04:00  PF8D3NWV0EP941UNREM9JO094PQNJDR9   \n",
       "\n",
       "                                               str2_0       a_1       b_1  \\\n",
       "timestamp                                                                   \n",
       "2020-01-01 00:00:00  8P63JISH1ZP47MIAZBI12V1JXGK58IX8  0.265880  0.903861   \n",
       "2020-01-01 00:01:00  DHR0FWQD06X1373HHDCZIH7XVYN4ATNP  0.431852  0.119335   \n",
       "2020-01-01 00:02:00  46YDT9RO6UUM9ZEVE537U2WH9M9YANX2  0.170822  0.954205   \n",
       "2020-01-01 00:03:00  X24BJX4X4H0SXXSTER0WY2OG8H51OPRP  0.437260  0.907608   \n",
       "2020-01-01 00:04:00  HPO5JJA877ORFQXB6NWIG2MCRF2G38YP  0.385516  0.230385   \n",
       "\n",
       "                     c_1 cat_1  ... c_3 cat_3  \\\n",
       "timestamp                       ...             \n",
       "2020-01-01 00:00:00   82  cat2  ...  13  cat4   \n",
       "2020-01-01 00:01:00   21  cat2  ...  73  cat2   \n",
       "2020-01-01 00:02:00   11  cat5  ...   6  cat2   \n",
       "2020-01-01 00:03:00   42  cat4  ...   5  cat4   \n",
       "2020-01-01 00:04:00   12  cat4  ...  72  cat4   \n",
       "\n",
       "                                               str1_3  \\\n",
       "timestamp                                               \n",
       "2020-01-01 00:00:00  NBVW8J14DZTVEDHISETW5GAK7EF17GXG   \n",
       "2020-01-01 00:01:00  HZRH5W38UBQR9OV6BV3FY8EMTP33HSPB   \n",
       "2020-01-01 00:02:00  NI4WLNG33HG36JSWYSPJR1A411L86UYM   \n",
       "2020-01-01 00:03:00  JN91A6OZKPZVOQ659KMWBCI0S314QDP3   \n",
       "2020-01-01 00:04:00  ZSX13ZKIUQ1JB9V5QTM05D7O8SQJFOIN   \n",
       "\n",
       "                                               str2_3       a_4       b_4 c_4  \\\n",
       "timestamp                                                                       \n",
       "2020-01-01 00:00:00  OEZF17L5NT64ZJB0KDDPXLGZ65OZAHSO  0.687856  0.678377  34   \n",
       "2020-01-01 00:01:00  HNUFOM1BLH8E30XVTFSFK13AZP23KX2Z  0.186750  0.882835  43   \n",
       "2020-01-01 00:02:00  P6BULVBOV581SOARU9QTSQK8WW4HRKKS  0.864973  0.355677  71   \n",
       "2020-01-01 00:03:00  JXZPV75E16IKN066HF1YEZN818R3DBA2  0.945028  0.491691  87   \n",
       "2020-01-01 00:04:00  9MVSK7KZ3FPJZYVKMOKQ9OF2UYZY2VJ6  0.328728  0.631368   3   \n",
       "\n",
       "                    cat_4                            str1_4  \\\n",
       "timestamp                                                     \n",
       "2020-01-01 00:00:00  cat4  TEJ5L414VFDEE80ZKN4MOTT55W84VIV1   \n",
       "2020-01-01 00:01:00  cat5  LAEIZ0WESDJ6JZ2E0M8LMQAH7ZBZEHY8   \n",
       "2020-01-01 00:02:00  cat5  3596BQIYQQ0FLWQDBRM9NFTRGJVOLPVH   \n",
       "2020-01-01 00:03:00  cat2  HF6EQ8TJ923PM3028ZER7812CV2M2HN4   \n",
       "2020-01-01 00:04:00  cat2  2VMO6KZ0NR5YYBMUUJ4LZ2GUGFT1PQSX   \n",
       "\n",
       "                                               str2_4  \n",
       "timestamp                                              \n",
       "2020-01-01 00:00:00  04ZMT2BQ1A9LDVL5C8FAMYIH5UWBU0UI  \n",
       "2020-01-01 00:01:00  GQT42GP69K4YZ50H00I695KRSEBX87BB  \n",
       "2020-01-01 00:02:00  AW7H0ELFQFU75NVEN5ZU5AK1FGV0LAG4  \n",
       "2020-01-01 00:03:00  AUGEW9I2R1L7EBJ1TJBK5V765S24SBKZ  \n",
       "2020-01-01 00:04:00  GGOND089HZEJEXNECS4RMFP5PFA7JNT6  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b27ea0-8cf0-44ac-b7d8-6148c8920072",
   "metadata": {},
   "source": [
    "The method `info(memory_usage='deep')` returns the column types and also gives the memory usage of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ad7916-7f9d-4644-a9cc-c1916832d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2102400 entries, 2020-01-01 00:00:00 to 2023-12-30 23:59:00\n",
      "Freq: min\n",
      "Data columns (total 30 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   a_0     float64\n",
      " 1   b_0     float64\n",
      " 2   c_0     int64  \n",
      " 3   cat_0   object \n",
      " 4   str1_0  object \n",
      " 5   str2_0  object \n",
      " 6   a_1     float64\n",
      " 7   b_1     float64\n",
      " 8   c_1     int64  \n",
      " 9   cat_1   object \n",
      " 10  str1_1  object \n",
      " 11  str2_1  object \n",
      " 12  a_2     float64\n",
      " 13  b_2     float64\n",
      " 14  c_2     int64  \n",
      " 15  cat_2   object \n",
      " 16  str1_2  object \n",
      " 17  str2_2  object \n",
      " 18  a_3     float64\n",
      " 19  b_3     float64\n",
      " 20  c_3     int64  \n",
      " 21  cat_3   object \n",
      " 22  str1_3  object \n",
      " 23  str2_3  object \n",
      " 24  a_4     float64\n",
      " 25  b_4     float64\n",
      " 26  c_4     int64  \n",
      " 27  cat_4   object \n",
      " 28  str1_4  object \n",
      " 29  str2_4  object \n",
      "dtypes: float64(10), int64(5), object(15)\n",
      "memory usage: 2.6 GB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48de37-16d9-4594-96b8-f2d05a4267ff",
   "metadata": {},
   "source": [
    "Write the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad11d68-d0a1-42c3-9014-8d17050c7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"data\").mkdir(parents=True,exist_ok=True)\n",
    "df.to_parquet(\"timeseries.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481fa5e-fc4c-446f-9202-b26d2f33cc76",
   "metadata": {},
   "source": [
    "## Load only useful data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98998232-70d3-4382-9fbb-9783b840d399",
   "metadata": {},
   "source": [
    "Image that you are interested only by "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f13358-91ba-4366-a25d-dce0b9f98c06",
   "metadata": {},
   "source": [
    "Imagine you're only interested in a subset of the dataset's columns `['a_0','a_1','cat_0','str1_0','str1_1']`. Then there are two ways to proceed: \n",
    " * either load the entire dataset and then filter out the columns you're interested in\n",
    " * or read only the columns you're interested in\n",
    "\n",
    "Compare the two loading methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecc772-1111-4ed9-87d3-8045d7850cfe",
   "metadata": {},
   "source": [
    "Look at the `read_parquet`method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f16893-9a94-4f23-bbff-8f25a4a513b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FilePath | ReadBuffer[bytes]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StorageOptions | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DtypeBackend | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilesystem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[tuple] | list[list[tuple]] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load a parquet object from the file path, returning a DataFrame.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path : str, path object or file-like object\n",
       "    String, path object (implementing ``os.PathLike[str]``), or file-like\n",
       "    object implementing a binary ``read()`` function.\n",
       "    The string could be a URL. Valid URL schemes include http, ftp, s3,\n",
       "    gs, and file. For file URLs, a host is expected. A local file could be:\n",
       "    ``file://localhost/path/to/table.parquet``.\n",
       "    A file URL can also be a path to a directory that contains multiple\n",
       "    partitioned parquet files. Both pyarrow and fastparquet support\n",
       "    paths to directories as well as file URLs. A directory path could be:\n",
       "    ``file://localhost/path/to/tables`` or ``s3://bucket/partition_dir``.\n",
       "engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n",
       "    Parquet library to use. If 'auto', then the option\n",
       "    ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n",
       "    behavior is to try 'pyarrow', falling back to 'fastparquet' if\n",
       "    'pyarrow' is unavailable.\n",
       "\n",
       "    When using the ``'pyarrow'`` engine and no storage options are provided\n",
       "    and a filesystem is implemented by both ``pyarrow.fs`` and ``fsspec``\n",
       "    (e.g. \"s3://\"), then the ``pyarrow.fs`` filesystem is attempted first.\n",
       "    Use the filesystem keyword with an instantiated fsspec filesystem\n",
       "    if you wish to use its implementation.\n",
       "columns : list, default=None\n",
       "    If not None, only these columns will be read from the file.\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
       "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
       "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
       "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
       "    details, and for more examples on storage options refer `here\n",
       "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
       "    highlight=storage_options#reading-writing-remote-files>`_.\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "use_nullable_dtypes : bool, default False\n",
       "    If True, use dtypes that use ``pd.NA`` as missing value indicator\n",
       "    for the resulting DataFrame. (only applicable for the ``pyarrow``\n",
       "    engine)\n",
       "    As new dtypes are added that support ``pd.NA`` in the future, the\n",
       "    output with this option will change to use those dtypes.\n",
       "    Note: this is an experimental option, and behaviour (e.g. additional\n",
       "    support dtypes) may change without notice.\n",
       "\n",
       "    .. deprecated:: 2.0\n",
       "\n",
       "dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
       "    Back-end data type applied to the resultant :class:`DataFrame`\n",
       "    (still experimental). Behaviour is as follows:\n",
       "\n",
       "    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
       "      (default).\n",
       "    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
       "      DataFrame.\n",
       "\n",
       "    .. versionadded:: 2.0\n",
       "\n",
       "filesystem : fsspec or pyarrow filesystem, default None\n",
       "    Filesystem object to use when reading the parquet file. Only implemented\n",
       "    for ``engine=\"pyarrow\"``.\n",
       "\n",
       "    .. versionadded:: 2.1.0\n",
       "\n",
       "filters : List[Tuple] or List[List[Tuple]], default None\n",
       "    To filter out data.\n",
       "    Filter syntax: [[(column, op, val), ...],...]\n",
       "    where op is [==, =, >, >=, <, <=, !=, in, not in]\n",
       "    The innermost tuples are transposed into a set of filters applied\n",
       "    through an `AND` operation.\n",
       "    The outer list combines these sets of filters through an `OR`\n",
       "    operation.\n",
       "    A single list of tuples can also be used, meaning that no `OR`\n",
       "    operation between set of filters is to be conducted.\n",
       "\n",
       "    Using this argument will NOT result in row-wise filtering of the final\n",
       "    partitions unless ``engine=\"pyarrow\"`` is also specified.  For\n",
       "    other engines, filtering is only performed at the partition level, that is,\n",
       "    to prevent the loading of some row-groups and/or files.\n",
       "\n",
       "    .. versionadded:: 2.1.0\n",
       "\n",
       "**kwargs\n",
       "    Any additional kwargs are passed to the engine.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.to_parquet : Create a parquet object that serializes a DataFrame.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> original_df = pd.DataFrame(\n",
       "...     {\"foo\": range(5), \"bar\": range(5, 10)}\n",
       "...    )\n",
       ">>> original_df\n",
       "   foo  bar\n",
       "0    0    5\n",
       "1    1    6\n",
       "2    2    7\n",
       "3    3    8\n",
       "4    4    9\n",
       ">>> df_parquet_bytes = original_df.to_parquet()\n",
       ">>> from io import BytesIO\n",
       ">>> restored_df = pd.read_parquet(BytesIO(df_parquet_bytes))\n",
       ">>> restored_df\n",
       "   foo  bar\n",
       "0    0    5\n",
       "1    1    6\n",
       "2    2    7\n",
       "3    3    8\n",
       "4    4    9\n",
       ">>> restored_df.equals(original_df)\n",
       "True\n",
       ">>> restored_bar = pd.read_parquet(BytesIO(df_parquet_bytes), columns=[\"bar\"])\n",
       ">>> restored_bar\n",
       "    bar\n",
       "0    5\n",
       "1    6\n",
       "2    7\n",
       "3    8\n",
       "4    9\n",
       ">>> restored_bar.equals(original_df[['bar']])\n",
       "True\n",
       "\n",
       "The function uses `kwargs` that are passed directly to the engine.\n",
       "In the following example, we use the `filters` argument of the pyarrow\n",
       "engine to filter the rows of the DataFrame.\n",
       "\n",
       "Since `pyarrow` is the default engine, we can omit the `engine` argument.\n",
       "Note that the `filters` argument is implemented by the `pyarrow` engine,\n",
       "which can benefit from multithreading and also potentially be more\n",
       "economical in terms of memory.\n",
       "\n",
       ">>> sel = [(\"foo\", \">\", 2)]\n",
       ">>> restored_part = pd.read_parquet(BytesIO(df_parquet_bytes), filters=sel)\n",
       ">>> restored_part\n",
       "    foo  bar\n",
       "0    3    8\n",
       "1    4    9\n",
       "\u001b[0;31mFile:\u001b[0m      ~/pyenvs/isae/lib/python3.10/site-packages/pandas/io/parquet.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pd.read_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5deac4-af5b-4854-bcec-8bd417cb2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['a_0','a_1','cat_0','str1_0','str1_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a99de2-b3d8-4b21-8cd7-0c1e1490c9a6",
   "metadata": {},
   "source": [
    "**Option 1**: Load the entire dataset and then filter out the columns you're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a2a6915-19d6-4d4e-825c-42ccce0ba8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d194de9-54d9-4d11-bdc4-55107db4a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 8360.64 MiB, increment: 3850.33 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit df_filter = pd.read_parquet(\"timeseries.parquet\")[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad3d7565-07e8-4501-a338-d9704c3ecd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.39 s, sys: 5.32 s, total: 12.7 s\n",
      "Wall time: 7.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>cat_0</th>\n",
       "      <th>str1_0</th>\n",
       "      <th>str1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>0.439790</td>\n",
       "      <td>0.265880</td>\n",
       "      <td>cat1</td>\n",
       "      <td>25CH60DTU73CZ67GR54GS83RD6N3ARPP</td>\n",
       "      <td>I2IPGR00OEV176DVCR80BJQGMILCL27U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:01:00</th>\n",
       "      <td>0.307949</td>\n",
       "      <td>0.431852</td>\n",
       "      <td>cat4</td>\n",
       "      <td>WU6SF4XH0N297NGVQRVT4HJPRHFYY3K0</td>\n",
       "      <td>SIO7HVCE7ST8D79QPC0UKBYSBJTFTG6K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:02:00</th>\n",
       "      <td>0.844226</td>\n",
       "      <td>0.170822</td>\n",
       "      <td>cat2</td>\n",
       "      <td>CXHNSBLSIIPQ9V5CBM1XA13APFKC4TX2</td>\n",
       "      <td>OXRZBU95BT844XTKK9GX0DQ7GJNFJZW9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:03:00</th>\n",
       "      <td>0.509127</td>\n",
       "      <td>0.437260</td>\n",
       "      <td>cat2</td>\n",
       "      <td>24PBVSV29Z015YFR8XR0FM88YBXWGP52</td>\n",
       "      <td>VNURGQNX1O89LBFOK0TSJKYK5MQ1E2G3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:04:00</th>\n",
       "      <td>0.845486</td>\n",
       "      <td>0.385516</td>\n",
       "      <td>cat5</td>\n",
       "      <td>PF8D3NWV0EP941UNREM9JO094PQNJDR9</td>\n",
       "      <td>GPC1DA2B8CS1J9APR76ATO0GJKUFIAK4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          a_0       a_1 cat_0  \\\n",
       "timestamp                                       \n",
       "2020-01-01 00:00:00  0.439790  0.265880  cat1   \n",
       "2020-01-01 00:01:00  0.307949  0.431852  cat4   \n",
       "2020-01-01 00:02:00  0.844226  0.170822  cat2   \n",
       "2020-01-01 00:03:00  0.509127  0.437260  cat2   \n",
       "2020-01-01 00:04:00  0.845486  0.385516  cat5   \n",
       "\n",
       "                                               str1_0  \\\n",
       "timestamp                                               \n",
       "2020-01-01 00:00:00  25CH60DTU73CZ67GR54GS83RD6N3ARPP   \n",
       "2020-01-01 00:01:00  WU6SF4XH0N297NGVQRVT4HJPRHFYY3K0   \n",
       "2020-01-01 00:02:00  CXHNSBLSIIPQ9V5CBM1XA13APFKC4TX2   \n",
       "2020-01-01 00:03:00  24PBVSV29Z015YFR8XR0FM88YBXWGP52   \n",
       "2020-01-01 00:04:00  PF8D3NWV0EP941UNREM9JO094PQNJDR9   \n",
       "\n",
       "                                               str1_1  \n",
       "timestamp                                              \n",
       "2020-01-01 00:00:00  I2IPGR00OEV176DVCR80BJQGMILCL27U  \n",
       "2020-01-01 00:01:00  SIO7HVCE7ST8D79QPC0UKBYSBJTFTG6K  \n",
       "2020-01-01 00:02:00  OXRZBU95BT844XTKK9GX0DQ7GJNFJZW9  \n",
       "2020-01-01 00:03:00  VNURGQNX1O89LBFOK0TSJKYK5MQ1E2G3  \n",
       "2020-01-01 00:04:00  GPC1DA2B8CS1J9APR76ATO0GJKUFIAK4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_filter = pd.read_parquet(\"timeseries.parquet\")[columns]\n",
    "df_filter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3c2cb-4ecc-4b54-8eb7-07136682a984",
   "metadata": {},
   "source": [
    "**Option 2**: Read only the columns you're interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1b0de0-cee9-4f8f-8bb6-912734a3e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b7d3c2-4289-469d-befb-fe9d4b5b52d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6119.67 MiB, increment: -334.02 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit df_filter = pd.read_parquet(\"timeseries.parquet\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854d372b-1119-44b7-a98c-35d3e1aa748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 s, sys: 1.18 s, total: 2.57 s\n",
      "Wall time: 1.83 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>cat_0</th>\n",
       "      <th>str1_0</th>\n",
       "      <th>str1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>0.439790</td>\n",
       "      <td>0.265880</td>\n",
       "      <td>cat1</td>\n",
       "      <td>25CH60DTU73CZ67GR54GS83RD6N3ARPP</td>\n",
       "      <td>I2IPGR00OEV176DVCR80BJQGMILCL27U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:01:00</th>\n",
       "      <td>0.307949</td>\n",
       "      <td>0.431852</td>\n",
       "      <td>cat4</td>\n",
       "      <td>WU6SF4XH0N297NGVQRVT4HJPRHFYY3K0</td>\n",
       "      <td>SIO7HVCE7ST8D79QPC0UKBYSBJTFTG6K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:02:00</th>\n",
       "      <td>0.844226</td>\n",
       "      <td>0.170822</td>\n",
       "      <td>cat2</td>\n",
       "      <td>CXHNSBLSIIPQ9V5CBM1XA13APFKC4TX2</td>\n",
       "      <td>OXRZBU95BT844XTKK9GX0DQ7GJNFJZW9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:03:00</th>\n",
       "      <td>0.509127</td>\n",
       "      <td>0.437260</td>\n",
       "      <td>cat2</td>\n",
       "      <td>24PBVSV29Z015YFR8XR0FM88YBXWGP52</td>\n",
       "      <td>VNURGQNX1O89LBFOK0TSJKYK5MQ1E2G3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:04:00</th>\n",
       "      <td>0.845486</td>\n",
       "      <td>0.385516</td>\n",
       "      <td>cat5</td>\n",
       "      <td>PF8D3NWV0EP941UNREM9JO094PQNJDR9</td>\n",
       "      <td>GPC1DA2B8CS1J9APR76ATO0GJKUFIAK4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          a_0       a_1 cat_0  \\\n",
       "timestamp                                       \n",
       "2020-01-01 00:00:00  0.439790  0.265880  cat1   \n",
       "2020-01-01 00:01:00  0.307949  0.431852  cat4   \n",
       "2020-01-01 00:02:00  0.844226  0.170822  cat2   \n",
       "2020-01-01 00:03:00  0.509127  0.437260  cat2   \n",
       "2020-01-01 00:04:00  0.845486  0.385516  cat5   \n",
       "\n",
       "                                               str1_0  \\\n",
       "timestamp                                               \n",
       "2020-01-01 00:00:00  25CH60DTU73CZ67GR54GS83RD6N3ARPP   \n",
       "2020-01-01 00:01:00  WU6SF4XH0N297NGVQRVT4HJPRHFYY3K0   \n",
       "2020-01-01 00:02:00  CXHNSBLSIIPQ9V5CBM1XA13APFKC4TX2   \n",
       "2020-01-01 00:03:00  24PBVSV29Z015YFR8XR0FM88YBXWGP52   \n",
       "2020-01-01 00:04:00  PF8D3NWV0EP941UNREM9JO094PQNJDR9   \n",
       "\n",
       "                                               str1_1  \n",
       "timestamp                                              \n",
       "2020-01-01 00:00:00  I2IPGR00OEV176DVCR80BJQGMILCL27U  \n",
       "2020-01-01 00:01:00  SIO7HVCE7ST8D79QPC0UKBYSBJTFTG6K  \n",
       "2020-01-01 00:02:00  OXRZBU95BT844XTKK9GX0DQ7GJNFJZW9  \n",
       "2020-01-01 00:03:00  VNURGQNX1O89LBFOK0TSJKYK5MQ1E2G3  \n",
       "2020-01-01 00:04:00  GPC1DA2B8CS1J9APR76ATO0GJKUFIAK4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_filter = pd.read_parquet(\"timeseries.parquet\",columns=columns)\n",
    "df_filter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1f300-4cf7-4f8f-8360-827f2641559f",
   "metadata": {},
   "source": [
    "You can use the magic command `%time` and `%memit` to compare the time and the memory usage of the two calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258528d6-1243-403a-a4c3-39bac7ef99e4",
   "metadata": {},
   "source": [
    "Not all the reading methods in Pandas has an option to read a subset of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52464758-568b-4b25-81cb-9296cb0a9011",
   "metadata": {},
   "source": [
    "### Use efficient datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc934a-fc02-4a50-8547-c6da2e24a093",
   "metadata": {},
   "source": [
    "The default pandas data types are not the most memory efficient. This is especially true for text data columns with relatively few unique values (commonly referred to as “low-cardinality” data). \n",
    "\n",
    "Using more efficient data types reduces the memory size of a dataframe, so you can store larger datasets in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c4ff9e-f1e9-4610-b4b8-8e0097e6a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"timeseries.parquet\",columns=['a_0','b_0','c_0','cat_0','str1_0','str2_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211102b1-cb30-4b49-8fa2-b433b2f7e387",
   "metadata": {},
   "source": [
    "Look at the data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4de5951-f755-4f8c-b8c2-b64ce9d08196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a_0       float64\n",
       "b_0       float64\n",
       "c_0         int64\n",
       "cat_0      object\n",
       "str1_0     object\n",
       "str2_0     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2b3d0-88e1-43fe-8fc1-8a9423eb8145",
   "metadata": {},
   "source": [
    "Look at the memory usage of the dataframe. The `memory_usage()` method returns the memory usage of each column in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f71fde-fe65-4ae8-8919-2307276bcc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index      16819200\n",
       "a_0        16819200\n",
       "b_0        16819200\n",
       "c_0        16819200\n",
       "cat_0     128246400\n",
       "str1_0    187113600\n",
       "str2_0    187113600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb224fb7-1f62-4829-b574-9642fa24d061",
   "metadata": {},
   "source": [
    "Compute the size of the dataframe. You should get the same result with the `info(memory_usage='deep')` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51724fd5-2972-4792-acc4-cd41e25d7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d16097-9270-48bb-8f68-91897871eb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543.3563232421875"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem = df.memory_usage(deep=True)\n",
    "mem.sum()/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "528cd0fd-8ecd-4af8-a0f3-69149e58fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2102400 entries, 2020-01-01 00:00:00 to 2023-12-30 23:59:00\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   a_0     float64\n",
      " 1   b_0     float64\n",
      " 2   c_0     int64  \n",
      " 3   cat_0   object \n",
      " 4   str1_0  object \n",
      " 5   str2_0  object \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 543.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd1dfd-8be3-4c67-8a21-89065c6671ed",
   "metadata": {},
   "source": [
    "The result of `memory_usage` show that the columns taking up much more memory are 'str1_0','str2_0','cat_0'. It seems normal for 'str1_0','str2_0' columns because those columns contains random strings. But 'cat_0' column has just a few unique values, so it’s a good candidate for converting to a pandas.Categorical. With a pandas.Categorical, we store each unique name once and use space-efficient integers to know which specific name is used in each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aca09c-3f16-4e4a-aff6-8836122b209b",
   "metadata": {},
   "source": [
    "First, we copy our dataframe to a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6170c8b5-2060-442b-a8c6-8b9b8d7dab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30fbc2-fe83-45f8-99b3-f9ba4f1effa0",
   "metadata": {},
   "source": [
    "Try to change to column type to Pandas.category using the `astype()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1384d57b-b028-47a8-bfa0-9d4ed6c6d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9408a4b-2bd7-4ee3-9f48-1f671e7bbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"cat_0\"] = df2[\"cat_0\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bfa8d-8a0b-4385-90a4-62f3f6afa212",
   "metadata": {},
   "source": [
    "Check with dtypes that the column type has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "722d13b4-2d65-4541-bb41-f18ed96eb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fd29f9c-f63c-436e-8cf2-acbd0b4e4652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a_0        float64\n",
       "b_0        float64\n",
       "c_0          int64\n",
       "cat_0     category\n",
       "str1_0      object\n",
       "str2_0      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981fd5bc-1cdb-492a-afba-6c69f1e048f1",
   "metadata": {},
   "source": [
    "Compute the memory usage of each column for this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91d87f5a-ca67-479c-baf2-521685834244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5246b518-3bec-474b-bb32-a18a44894ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index      16819200\n",
       "a_0        16819200\n",
       "b_0        16819200\n",
       "c_0        16819200\n",
       "cat_0       2102877\n",
       "str1_0    187113600\n",
       "str2_0    187113600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f8b1c-5c89-42f1-8f9a-7540b31dda8b",
   "metadata": {},
   "source": [
    "We can go a bit further and downcast the numeric columns to their smallest types using pandas.to_numeric(). The \"c_0\" column contains number between 0 and 100. So it can be downcast to unsigned. If float precision is sufficient for columns 'a_0' et 'b_0', it is also possible to downcast to float. Be careful when you downcast, you lose precision and so you can propagate error during the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73729db4-b492-4a01-9e98-c1dd97cebc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25164d1d-21be-4e84-b22d-126c0e5cc374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"c_0\"] = pd.to_numeric(df2[\"c_0\"], downcast=\"unsigned\")\n",
    "df2[[\"a_0\", \"b_0\"]] = df2[[\"a_0\", \"b_0\"]].apply(pd.to_numeric, downcast=\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35731db4-4236-4e48-b570-43a896244ed2",
   "metadata": {},
   "source": [
    "Check the types and the memory usage of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3ecbc2b-ebf4-468b-a546-7543f2332abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbd83c67-c21a-4e6e-aa88-c99513895471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a_0        float32\n",
       "b_0        float32\n",
       "c_0          uint8\n",
       "cat_0     category\n",
       "str1_0      object\n",
       "str2_0      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ef4b339-15c0-4899-bdba-b47087212d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index      16819200\n",
       "a_0         8409600\n",
       "b_0         8409600\n",
       "c_0         2102400\n",
       "cat_0       2102877\n",
       "str1_0    187113600\n",
       "str2_0    187113600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a4ac01-7166-4db0-b508-c6eebce7dac4",
   "metadata": {},
   "source": [
    "Compute the memory reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccce61e8-3e56-4d08-9d6a-17234952af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db304215-134a-4ad9-a1b8-0195c271eb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "reduction = df2.memory_usage(deep=True).sum() / df.memory_usage(deep=True).sum()\n",
    "print(f\"{reduction:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91e607-4e97-4902-a811-ab25c35c54d0",
   "metadata": {},
   "source": [
    "# Use chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329486b-2381-4769-8c70-fd22ce5b05cf",
   "metadata": {},
   "source": [
    "Some problem are embarrasingly parallel and so can be processed with chunking, which means by splitting a large problem into a bunch of small problems. \n",
    "For example, converting an big file into several smaller files and repeating the processing for each file in a directory. \n",
    "As long as each chunk fits in memory, you can work with datasets that are much larger than memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad9fd67d-efa2-44a5-8d83-6b51ec7ae600",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "starts = [f\"20{i:>02d}-01-01\" for i in range(N)]\n",
    "ends = [f\"20{i:>02d}-12-31\" for i in range(N)]\n",
    "pathlib.Path(\"data/timeseries\").mkdir(parents=True,exist_ok=True)\n",
    "for i, (start, end) in enumerate(zip(starts, ends)):\n",
    "    ts = make_timeseries(start=start, end=end, freq=\"1min\", seed=i)\n",
    "    ts.to_parquet(f\"data/timeseries/ts-{i:0>2d}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f9e5b-1bc2-43c2-bb0c-eb9d597cb05b",
   "metadata": {},
   "source": [
    "Count the occurence of the values in the \"c\" column for all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eada80ef-04b5-4df4-84e9-82665a4e8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46e66607-c707-421d-90c2-afac8ac8a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.66 s, sys: 2.32 s, total: 5.98 s\n",
      "Wall time: 4.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "c\n",
       "1     63221\n",
       "2     63234\n",
       "3     63501\n",
       "4     64154\n",
       "5     63608\n",
       "      ...  \n",
       "95    63308\n",
       "96    63758\n",
       "97    63770\n",
       "98    63447\n",
       "99    63236\n",
       "Length: 99, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "files = pathlib.Path(\"data/timeseries/\").glob(\"ts*.parquet\")\n",
    "counts = pd.Series(dtype=int)\n",
    "\n",
    "for path in files:\n",
    "    df = pd.read_parquet(path)\n",
    "    counts = counts.add(df[\"c\"].value_counts(), fill_value=0)\n",
    "\n",
    "counts.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e4f2b-d3df-4321-8612-67cdb6bba80b",
   "metadata": {},
   "source": [
    "Some readers, like pandas.read_csv(), offer parameters to control the chunksize when reading a single file. \n",
    "In that case, it is possible to read a file chunk by chunk in order to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51621b7b-229e-4bd1-999f-756dfd832eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_timeseries(start=\"2023-01-01\", end=\"2023-12-31\", freq=\"1min\", seed=10)\n",
    "df.to_csv(\"data/timeseries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aed5f3-c89a-4e08-8d08-72dbeede892f",
   "metadata": {},
   "source": [
    "Try to count the occurence of the values in the \"c\" column for the CSV file by process it chunk by chunk. You need to use the parameter `chunksize` in the `read_csv`method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89b07599-da5d-4a5c-ae8c-cda2dc976f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9a762f7-dfc2-4e91-9b7e-71381d4e1aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c\n",
       "1     5338\n",
       "2     5330\n",
       "3     5334\n",
       "4     5272\n",
       "5     5282\n",
       "      ... \n",
       "95    5197\n",
       "96    5215\n",
       "97    5344\n",
       "98    5296\n",
       "99    5212\n",
       "Length: 99, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.Series(dtype=int)\n",
    "with pd.read_csv(\"data/timeseries.csv\",chunksize=1000) as reader:\n",
    "    for chunk in reader:\n",
    "        counts = counts.add(chunk[\"c\"].value_counts(), fill_value=0)\n",
    "\n",
    "counts.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d4c314c-b9c3-49b8-963f-52858ae9147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6178.48 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "with pd.read_csv(\"data/timeseries.csv\",chunksize=1000) as reader:\n",
    "    for chunk in reader:\n",
    "        counts = counts.add(chunk[\"c\"].value_counts(), fill_value=0)\n",
    "\n",
    "counts.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf54eb9e-61ca-48b8-87ff-f0fea449986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6654.18 MiB, increment: 113.72 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "df = pd.read_csv(\"data/timeseries.csv\")\n",
    "df[\"c\"].value_counts().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b857b1-9883-4c3c-9a0b-b206811e6baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
