{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d265295f-455b-444b-a889-81d90dba6495",
   "metadata": {},
   "source": [
    "# Pandas: scaling to large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb0843-8a3e-4ad9-a086-1b8809199dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b601349-90df-491a-9d81-1efa8b522a94",
   "metadata": {},
   "source": [
    "Create a large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913fa68-8958-4964-b167-c286d12f1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def gen_random_string(length:int=32) -> str:\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))\n",
    "    \n",
    "def make_timeseries(start=\"2000-01-01\", end=\"2000-12-31\", freq=\"1D\", seed=None):\n",
    "\n",
    "    index = pd.date_range(start=start, end=end, freq=freq, name=\"timestamp\")\n",
    "    n = len(index)\n",
    "    np.random.seed = seed\n",
    "    columns = {\n",
    "        'cat': np.random.choice(['cat1','cat2','cat3','cat4','cat5'],n),\n",
    "        'str1':[gen_random_string() for _ in range(n)],\n",
    "        'str2':[gen_random_string() for _ in range(n)],\n",
    "        'a': np.random.rand(n),\n",
    "        'b': np.random.rand(n),\n",
    "        'c': np.random.randint(1,100,n),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(columns, index=index, columns=sorted(columns))\n",
    "    if df.index[-1] == end:\n",
    "        df = df.iloc[:-1]\n",
    "    return df\n",
    "\n",
    "timeseries = [\n",
    "    make_timeseries(start=datetime(2020,1,1), end=datetime(2023,12,31), freq='1min', seed=10).rename(columns=lambda x: f\"{x}_{i}\")\n",
    "    for i in range(5)\n",
    "]\n",
    "df = pd.concat(timeseries, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed3e1c-0dd5-4793-9f8f-78bd43591dec",
   "metadata": {},
   "source": [
    "Print the fisrt rows to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a1a09-8469-4658-b37e-794bf63e6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b27ea0-8cf0-44ac-b7d8-6148c8920072",
   "metadata": {},
   "source": [
    "The method `info(memory_usage='deep')` returns the column types and also gives the memory usage of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad7916-7f9d-4644-a9cc-c1916832d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48de37-16d9-4594-96b8-f2d05a4267ff",
   "metadata": {},
   "source": [
    "Write the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad11d68-d0a1-42c3-9014-8d17050c7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"data\").mkdir(parents=True,exist_ok=True)\n",
    "df.to_parquet(\"timeseries.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481fa5e-fc4c-446f-9202-b26d2f33cc76",
   "metadata": {},
   "source": [
    "## Load only useful data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98998232-70d3-4382-9fbb-9783b840d399",
   "metadata": {},
   "source": [
    "Image that you are interested only by "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f13358-91ba-4366-a25d-dce0b9f98c06",
   "metadata": {},
   "source": [
    "Imagine you're only interested in a subset of the dataset's columns `['a_0','a_1','cat_0','str1_0','str1_1']`. Then there are two ways to proceed: \n",
    " * either load the entire dataset and then filter out the columns you're interested in\n",
    " * or read only the columns you're interested in\n",
    "\n",
    "Compare the two loading methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecc772-1111-4ed9-87d3-8045d7850cfe",
   "metadata": {},
   "source": [
    "Look at the `read_parquet`method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f16893-9a94-4f23-bbff-8f25a4a513b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.read_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5deac4-af5b-4854-bcec-8bd417cb2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['a_0','a_1','cat_0','str1_0','str1_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a99de2-b3d8-4b21-8cd7-0c1e1490c9a6",
   "metadata": {},
   "source": [
    "**Option 1**: Load the entire dataset and then filter out the columns you're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a6915-19d6-4d4e-825c-42ccce0ba8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3c2cb-4ecc-4b54-8eb7-07136682a984",
   "metadata": {},
   "source": [
    "**Option 2**: Read only the columns you're interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b0de0-cee9-4f8f-8bb6-912734a3e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1f300-4cf7-4f8f-8360-827f2641559f",
   "metadata": {},
   "source": [
    "You can use the magic command `%time` and `%memit` to compare the time and the memory usage of the two calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258528d6-1243-403a-a4c3-39bac7ef99e4",
   "metadata": {},
   "source": [
    "Not all the reading methods in Pandas has an option to read a subset of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52464758-568b-4b25-81cb-9296cb0a9011",
   "metadata": {},
   "source": [
    "### Use efficient datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc934a-fc02-4a50-8547-c6da2e24a093",
   "metadata": {},
   "source": [
    "The default pandas data types are not the most memory efficient. This is especially true for text data columns with relatively few unique values (commonly referred to as “low-cardinality” data). \n",
    "\n",
    "Using more efficient data types reduces the memory size of a dataframe, so you can store larger datasets in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4ff9e-f1e9-4610-b4b8-8e0097e6a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"timeseries.parquet\",columns=['a_0','b_0','c_0','cat_0','str1_0','str2_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211102b1-cb30-4b49-8fa2-b433b2f7e387",
   "metadata": {},
   "source": [
    "Look at the data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de5951-f755-4f8c-b8c2-b64ce9d08196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2b3d0-88e1-43fe-8fc1-8a9423eb8145",
   "metadata": {},
   "source": [
    "Look at the memory usage of the dataframe. The `memory_usage()` method returns the memory usage of each column in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f71fde-fe65-4ae8-8919-2307276bcc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb224fb7-1f62-4829-b574-9642fa24d061",
   "metadata": {},
   "source": [
    "Compute the size of the dataframe. You should get the same result with the `info(memory_usage='deep')` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51724fd5-2972-4792-acc4-cd41e25d7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd1dfd-8be3-4c67-8a21-89065c6671ed",
   "metadata": {},
   "source": [
    "The result of `memory_usage` show that the columns taking up much more memory are 'str1_0','str2_0','cat_0'. It seems normal for 'str1_0','str2_0' columns because those columns contains random strings. But 'cat_0' column has just a few unique values, so it’s a good candidate for converting to a pandas.Categorical. With a pandas.Categorical, we store each unique name once and use space-efficient integers to know which specific name is used in each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aca09c-3f16-4e4a-aff6-8836122b209b",
   "metadata": {},
   "source": [
    "First, we copy our dataframe to a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170c8b5-2060-442b-a8c6-8b9b8d7dab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30fbc2-fe83-45f8-99b3-f9ba4f1effa0",
   "metadata": {},
   "source": [
    "Try to change to column type to Pandas.category using the `astype()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384d57b-b028-47a8-bfa0-9d4ed6c6d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bfa8d-8a0b-4385-90a4-62f3f6afa212",
   "metadata": {},
   "source": [
    "Check with dtypes that the column type has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d13b4-2d65-4541-bb41-f18ed96eb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981fd5bc-1cdb-492a-afba-6c69f1e048f1",
   "metadata": {},
   "source": [
    "Compute the memory usage of each column for this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d87f5a-ca67-479c-baf2-521685834244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f8b1c-5c89-42f1-8f9a-7540b31dda8b",
   "metadata": {},
   "source": [
    "We can go a bit further and downcast the numeric columns to their smallest types using pandas.to_numeric(). The \"c_0\" column contains number between 0 and 100. So it can be downcast to unsigned. If float precision is sufficient for columns 'a_0' et 'b_0', it is also possible to downcast to float. Be careful when you downcast, you lose precision and so you can propagate error during the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73729db4-b492-4a01-9e98-c1dd97cebc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35731db4-4236-4e48-b570-43a896244ed2",
   "metadata": {},
   "source": [
    "Check the types and the memory usage of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecbc2b-ebf4-468b-a546-7543f2332abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a4ac01-7166-4db0-b508-c6eebce7dac4",
   "metadata": {},
   "source": [
    "Compute the memory reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce61e8-3e56-4d08-9d6a-17234952af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91e607-4e97-4902-a811-ab25c35c54d0",
   "metadata": {},
   "source": [
    "# Use chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329486b-2381-4769-8c70-fd22ce5b05cf",
   "metadata": {},
   "source": [
    "Some problem are embarrasingly parallel and so can be processed with chunking, which means by splitting a large problem into a bunch of small problems. \n",
    "For example, converting an big file into several smaller files and repeating the processing for each file in a directory. \n",
    "As long as each chunk fits in memory, you can work with datasets that are much larger than memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fd67d-efa2-44a5-8d83-6b51ec7ae600",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "starts = [f\"20{i:>02d}-01-01\" for i in range(N)]\n",
    "ends = [f\"20{i:>02d}-12-31\" for i in range(N)]\n",
    "pathlib.Path(\"data/timeseries\").mkdir(parents=True,exist_ok=True)\n",
    "for i, (start, end) in enumerate(zip(starts, ends)):\n",
    "    ts = make_timeseries(start=start, end=end, freq=\"1min\", seed=i)\n",
    "    ts.to_parquet(f\"data/timeseries/ts-{i:0>2d}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f9e5b-1bc2-43c2-bb0c-eb9d597cb05b",
   "metadata": {},
   "source": [
    "Count the occurence of the values in the \"c\" column for all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada80ef-04b5-4df4-84e9-82665a4e8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e4f2b-d3df-4321-8612-67cdb6bba80b",
   "metadata": {},
   "source": [
    "Some readers, like pandas.read_csv(), offer parameters to control the chunksize when reading a single file. \n",
    "In that case, it is possible to read a file chunk by chunk in order to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51621b7b-229e-4bd1-999f-756dfd832eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_timeseries(start=\"2023-01-01\", end=\"2023-12-31\", freq=\"1min\", seed=10)\n",
    "df.to_csv(\"data/timeseries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aed5f3-c89a-4e08-8d08-72dbeede892f",
   "metadata": {},
   "source": [
    "Try to count the occurence of the values in the \"c\" column for the CSV file by process it chunk by chunk. You need to use the parameter `chunksize` in the `read_csv`method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b07599-da5d-4a5c-ae8c-cda2dc976f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b857b1-9883-4c3c-9a0b-b206811e6baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
